{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebe9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf56949",
   "metadata": {},
   "source": [
    "### Text to Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7c16ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HEY, DID YOU KNOW THAT THE SUMMER BREAK IS COMING? AMAZING RIGHT!! IT'S ONLY 5 DAYS MORE !!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_uppercase(text):\n",
    "    return text.upper()\n",
    "\n",
    "input_str = \"Hey, did you know that the summer break is coming? Amazing right!! It's only 5 days more !!\"\n",
    "text_uppercase(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf69fea",
   "metadata": {},
   "source": [
    "### Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980ad8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey, did you know that the summer break is coming? amazing right!! it's only 5 days more !!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "input_str = \"HEY, DID YOU KNOW THAT THE SUMMER BREAK IS COMING? AMAZING RIGHT!! IT'S ONLY 5 DAYS MORE !!\"\n",
    "text_lowercase(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55419f38",
   "metadata": {},
   "source": [
    "### Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610c48b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are  balls in this bag, and  in the other.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "input_str = \"There are 3 balls in this bag, and 12 in the other.\"\n",
    "remove_numbers(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a19255",
   "metadata": {},
   "source": [
    "### Converting Figures into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e419ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three balls in this bag, and twelve in the other one.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "def convert_number(text):\n",
    "    temp_str = text.split()\n",
    "    new_string = []\n",
    "    \n",
    "    for word in temp_str:\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "            \n",
    "        else:\n",
    "            new_string.append(word)\n",
    "                \n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str\n",
    "    \n",
    "input_str = 'There are 3 balls in this bag, and 12 in the other one.'\n",
    "convert_number(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbdb528",
   "metadata": {},
   "source": [
    "### Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9b8c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey did you know that the summer break is coming Amazing right Its only 5 more days '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str = \"Hey, did you know that the summer break is coming? Amazing right!! It's only 5 more days !!\"\n",
    "remove_punctuations(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b685061",
   "metadata": {},
   "source": [
    "### Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447b3223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we don't need the given questions\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "    \n",
    "input_str = \"  we don't need  the given questions\"\n",
    "remove_whitespace(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80cfe2",
   "metadata": {},
   "source": [
    "### Remove Default Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a82f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
    "remove_stopwords(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0c79d",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de6dc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'typp',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithm and many typpes of processes'\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4064f",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c5b3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\HP/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\HP/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'use',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'many',\n",
       " 'typpes',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos = 'v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "text = 'data science uses scientific methods algorithm and many typpes of processes'\n",
    "lemmatize_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083da75",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a912ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('really', 'RB'),\n",
       " ('scared', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('right', 'RB'),\n",
       " ('there', 'EX')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def pos_tagging(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagging(\"You really scared me right there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52668d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to C:\\Users\\HP/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c153",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64b6fbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def chunking(text, grammar):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    tree.draw()\n",
    "    \n",
    "sentence = 'the little yellow bird is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0b14f",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28805af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Sunbo/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  (PERSON Microsoft/NNP)\n",
      "  so/RB\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Switzerland/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  meetup/NN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\HP/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    \n",
    "    print(ne_chunk(word_pos))\n",
    "\n",
    "text = 'Sunbo works for Microsoft so he went to Switzerland for a meetup'\n",
    "named_entity_recognition(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
